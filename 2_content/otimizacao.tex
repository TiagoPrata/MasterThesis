\newacronym{sujeitoa}{s.a.}{sujeito a}
\chapter{Otimização}
\label{cap_otimizacao}

\section{O problema da otimização}

Segundo \citeonline{Haugen2018} normalmente problemas de otimização são apresentados como problemas de minimização, como: "Encontre o valor ótimo de $x$ que minimize a \textit{função objetivo} $f(x)$, levando em consideração qualquer restrição sobre $x$ ou em função de $x$. A solução ótima é indicada por \simbolo{xopt}{$ x_{opt} $}{Valor ótimo de $x$ para minimizar $f(x)$}" \cite{Haugen2018}.

\citeonline{Haugen2018} ainda mostra que há várias formas de formular matematicamente um problema de otimização (minimização), mas que de forma geral, dado um modelo matemático $M$, é possível representá-lo como a minimização de $x$ para uma função $f(x)$, ou seja:

\begin{equation}
	\min_{x} f(x)
\end{equation}

sujeto a (também denotado por "\acrshort{sujeitoa}") restrições, que podem ser na forma de:

\begin{itemize}
\item Restrições de desigualdade:
	\begin{equation}
	\label{equ_min_restr_desigualdade}
	g(x) \leq 0
	\end{equation}
	onde $g$ pode ser uma função linear ou não-linear.
	
\item Restrições de igualdade:
	\begin{equation}
	\label{equ_min_restr_igualdade}
	h(x) = 0
	\end{equation}
	onde $h$ pode ser uma função linear ou não-linear de $x$.
	
\item Limites superiores e inferiores
	\begin{equation}
	\label{equ_min_limites}
	x_{li} \leq x \leq x_{ls}
	\end{equation}
	Onde $li$ e $ls$ indicam `limite inferior' e `limite superior', respectivamente.
\end{itemize}

Sendo que as equações \ref{equ_min_restr_desigualdade} e \ref{equ_min_restr_igualdade} definem restrições na relação entre as variáveis de otimização, enquanto \ref{equ_min_limites} define as regiões limites destas mesmas variáveis.

Existem diversas métodos para encontrar a solução ótima para um problema de otimização e a sessão a seguir irá mostrar exemplos e métodos numéricos simples que nos permitirão entender em maiores detalhes como um problema de minimização pode ser resolvido. No \autoref{cap_mpc} faremos uso das minimizações para compreender como o \acrshort{mpc} calcula valores ótimos dadas determinadas restrições em um dado horizonte de controle, pois uma maior compreensão sobre problemas de minimização pode fazer grande diferença no entendimento do controle \acrshort{mpc} em si.

\section{Algoritmos de otimização}

\subsection{Método de pesquisa em grade}

O método apresentado nesta sessão não aplicável a praticamente nenhum problema real devido a sua ineficiência computacional, porém ele ajuda a ilustrar o objetivo de todo o método numérico voltado para minimização.

Este método consiste em testar todos os valores de todas as variáveis (em um conjunto de dados definido) para verificar qual combinação minimiza a função objetivo, ou seja, testar todos os valores possíveis para $x(1)$, $x(2)$, $x(3)$, $...$, $x(n)$ com o objetivo de encontrar o \gls{xopt}, valor de $x$ que minimiza $f$.

No caso de um único $x$, um laço condicional simples poderia testar todos os valores da função objetivo. Para ilustrar essa ideia o algorítmo \ref{alg_grid_search_scalar}, em Python, mostra como a função $f(x)$ abaixo poderia ser computada, caso o intervalo de teste de $x$ fosse igual 100, ou seja, $N = 100$.

\begin{equation}
	\label{equ_grid_search_scalar}
	f(x) = 0,00232x^4 - 0,111x^3 + 1,8x^2 - 11,6x + 34,4
\end{equation}

Sendo que:
\[	2 \leq x \leq 22 \]

O intervalo $N = 100$ indica que serão analisados 100 valores entre $2$ e $22$.

\lstinputlisting[	
	caption={[Pesquisa em grade com número escalar]
			 Pesquisa em grade com número escalar \\
		     Fonte: Autor},
	label={alg_grid_search_scalar},
	language=Python,
	style=Python_lang]
	{./4_Algorithms/grid_search_scalar.py}

A figura \ref{fig_grid_search_scalar} plotada a partir do código acima mostra, destacado em vermelho, o valor de $x$ onde a $f(x)$ apresentava seu menor valor. Repare que o gráfico apresenta dois vales distintos: um deles é o já mencionado destaque em vermelho, onde o valor $x$ é $18,76$ e outro onde $x$ vale aproximadamente $5,5$. O vale do gráfico onde o valor de $x$ produz o menor valor de $f(x)$ é conhecido como \textit{mínimo global}, todos os outros são \textit{mínimos locais}, pois são os valores mínimos da função apenas para uma região limitada.

Algorítmos que buscam encontrar o valor mínimo de uma função podem erroneamente convergir para mínimos locais. O método de pesquisa em grade não é um desses algorítmos, pois ao verificar todos os valores de $x$ ele sempre encontrará o valor de $x$ que minimiza a função objetivo, porém nas próximas sessões serão apresentados métodos que, apesar de serem mais eficientes computacionalmente, podem tender para mínimos locais.
	
\begin{figure}
	\includegraphics[scale=1]{./5_images/fig_grid_search_scalar.pdf} 
	\caption{Pesquisa em grade com número escalar}
	\begin{center}
		\makebox[\width]{Fonte: Autor}
	\end{center}
	\label{fig_grid_search_scalar}
\end{figure}

Ainda neste método, casos onde há uma maior quantidade de $x$ ($x_1$, $x_2$, \dots) deve-se utilizar laços aninhados para que a varredura de todas as possibilidades possa ser feita. 

Como exemplo, minimizemos a equação \ref{equ_grid_search_vect_no_bounds}, sendo $0 \leq x_1 \leq 2$ e $1 \leq x_2 \leq 3$.

\begin{equation}
	\label{equ_grid_search_vect_no_bounds}
	f(x) = (x_1 - 1)^2 + (x_2 - 2)^2 + 0,5
\end{equation}

Neste caso, sem muito esforço notamos que $f_{min} = 0,5$, $x_{1_{opt}} = 1$ e $x_{2_{opt}} = 2$.\footnote{O algorítmo em Python para este cálculo pode ser encontrado no \autoref{cap_algoritmos_extras}, algorítmo \ref{alg_grid_search_vectorial}.} Porém se a restrição da inequação \ref{equ_grid_search_vect_bounds} for aplicada encontramos os valores descritos em \ref{equ_grid_search_vect_bounds_output}.

\begin{equation}
	\label{equ_grid_search_vect_bounds}
	f(x) = x_1 - x_2 + 1,5 \leq 0
\end{equation}

\begin{subequations}
	\label{equ_grid_search_vect_bounds_output}
	\begin{align}
		f_{min} = 0,628 \\
		x_{1_{opt}} = 0,748 \\
		x_{2_{opt}} = 2,25
	\end{align}
\end{subequations}

As figuras \ref{fig_grid_search_vectorial_nobounds} e \ref{fig_grid_search_vectorial_withbounds} mostram a função custo (outra forma de chamarmos a função objetivo) sem restrição e com a restrição descrita pela inequação \ref{equ_min_restr_desigualdade}, respectivamente.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{./5_images/fig_grid_search_vectorial1.png} 
		\caption{Pesquisa em grade com duas variáveis e sem restrição}
		\label{fig_grid_search_vectorial_nobounds}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{./5_images/fig_grid_search_vectorial2.png} 
		\caption{Pesquisa em grade com duas variáveis e com restrição}  
		\label{fig_grid_search_vectorial_withbounds}		      
	\end{minipage}
\end{figure}

\subsection{Método de busca de descidas mais íngrimes}

steepest descent method

\subsection{Método de Newton}

método de newton

\subsection{Otimizadores de programação não-linear (NLP)}

NLP

\section{Algumas aplicações de otimização}

\subsection{Estimação de parâmetros}

estimação de param

\subsection{Moving Horizon Estimation}

MHE

\subsection{Model Predictive Control}

MPC